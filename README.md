# Website Mirror

A powerful CLI utility written in Rust for mirroring websites by downloading static copies of HTML, CSS, images, and JavaScript files.

## ğŸ”§ Summary

I've successfully created a comprehensive website mirroring CLI utility in Rust 1.89.0 that addresses all your requirements:

### âœ… **Features Implemented:**

1. **CLI-based utility** with comprehensive command-line options
2. **Static website mirroring** - downloads HTML, CSS, images, and JavaScript files
3. **Link conversion** - automatically converts links to work locally
4. **Directory management** - creates necessary directories automatically
5. **SSL certificate handling** - built-in SSL support with rustls
6. **File extension adjustment** - automatically adds .html extension for HTML content
7. **Robots.txt bypass** - `--ignore-robots` flag to ignore robots.txt restrictions
8. **Zero 404 guarantee** - ALL media files (images, CSS, JS) are automatically downloaded from any site to ensure pages render properly offline
9. **External resource download** - `--download-external` flag for additional external resources
10. **Full recursive mirroring** - `--full-mirror` flag for comprehensive mirroring with all options enabled
11. **Smart download cache** - each unique file is downloaded only once, preventing duplicates and improving efficiency
12. **Enhanced resource logging** - clear visibility into what types of resources are being processed and downloaded
13. **External image URL resolution** - Automatically converts external CDN image URLs to local paths in HTML content
14. **Priority-based processing** - CSS/JS first, then HTML, then images for optimal offline rendering
15. **WebP image conversion** - Automatically converts JPEG/PNG images to WebP format for better compression

### ğŸ”§ **Key Components:**

- **`main.rs`** - CLI application entry point
- **`cli.rs`** - Command-line argument parsing with clap
- **`downloader.rs`** - Core mirroring logic with HTTP client and resource management
- **`html_parser.rs`** - HTML parsing and resource extraction
- **`file_manager.rs`** - File system operations and directory management

## Features

- ğŸš€ **Fast & Efficient**: Built with Rust for high performance
- ğŸ”— **Smart Link Conversion**: Automatically converts links to work locally
- ğŸ“ **Directory Management**: Creates necessary directories automatically
- ğŸ”’ **SSL Support**: Handles SSL certificates and secure connections
- ğŸ¤– **Robots.txt Bypass**: Option to ignore robots.txt restrictions
- ğŸŒ **External Resource Download**: Downloads CSS, JS, and images from external domains (AWS S3, CDNs, etc.)
- âš¡ **Concurrent Downloads**: Configurable concurrent download limits
- ğŸ“Š **Progress Tracking**: Real-time progress indicators and status updates
- ğŸ¯ **Depth Control**: Configurable crawling depth limits
- ğŸ’¾ **Smart Download Cache**: Prevents duplicate downloads and improves efficiency
- ğŸ” **Enhanced Resource Logging**: Clear visibility into resource types and download status
- ğŸ–¼ï¸ **External Image Resolution**: Automatically converts external CDN image URLs to local paths
- âš¡ **Priority Processing**: CSS/JS first, then HTML, then images for optimal offline rendering
- ğŸ–¼ï¸ **WebP Conversion**: Automatically converts JPEG/PNG images to WebP for better compression

## ğŸ¯ **Zero 404 Guarantee**

The utility now ensures that **every mirrored page will render completely offline without any 404 errors**. Here's how:

- **Images**: All `<img>` tags, background images from CSS, and inline styles are downloaded regardless of their hosting location
- **CSS**: All stylesheets and their referenced resources are downloaded
- **JavaScript**: All script files are downloaded
- **External Resources**: Media files from CDNs, AWS S3, or any other external domain are automatically downloaded

This means you can mirror a site and be confident it will work perfectly offline, even if it uses external resources from multiple domains.

## ğŸ¯ **Resource Type Filtering**

The `--only-resources` flag allows you to mirror only specific types of resources without downloading HTML pages:

- **`--only-resources images`** - Download only images (PNG, JPG, GIF, SVG, etc.)
- **`--only-resources css`** - Download only CSS files
- **`--only-resources js`** - Download only JavaScript files  
- **`--only-resources html`** - Download only HTML pages
- **`--only-resources images,css,js`** - Download images, CSS, and JavaScript (no HTML)

This is useful for:
- **Asset Caching**: Download only media files for offline use
- **Style/Function Caching**: Cache CSS and JS without full page mirroring
- **Selective Mirroring**: Focus on specific resource types for analysis
- **Bandwidth Optimization**: Skip HTML content when only assets are needed

## ğŸ–¼ï¸ **WebP Image Conversion**

The `--convert-to-webp` flag automatically converts JPEG and PNG images to WebP format during mirroring:

- **Automatic Conversion**: JPEG (.jpg, .jpeg) and PNG (.png) files are converted to WebP
- **Quality Optimization**: Uses quality 80/100 for optimal balance between file size and visual quality
- **File Size Reduction**: Typically reduces image file sizes by 25-50% while maintaining visual quality
- **HTML Updates**: Automatically updates HTML content to reference the new .webp files
- **Fallback Support**: If conversion fails, the original image is preserved

**Benefits:**
- **Smaller Storage**: Reduced disk space usage for mirrored sites
- **Faster Loading**: Smaller files load faster in browsers
- **Modern Format**: WebP is supported by all modern browsers
- **Bandwidth Savings**: Reduced transfer sizes for hosted mirrors

## ğŸ”§ **Recent Fixes & Improvements**

### **External Image URL Resolution (Fixed)**
- **Issue**: External images from CDNs (AWS S3, Cloudflare, etc.) were being downloaded but their URLs in HTML were not being updated to use local paths
- **Solution**: Enhanced HTML content processing to ensure all downloaded resources (CSS, JS, images) have their URLs updated to local paths
- **Result**: Mirrored sites now work completely offline with all images displaying correctly from local copies
- **Example**: AWS S3 images like `http://com.mykonosbiennale.static.s3.amazonaws.com/...` are now properly converted to local paths like `mykonos-biennale-cache05/fb/...`

### **Priority-Based Resource Processing**
- **CSS and JavaScript files** are downloaded first (Critical priority)
- **HTML pages** are queued for crawling (High priority)  
- **Images and other resources** are downloaded last (Normal priority)
- **Background images** from CSS are automatically extracted and downloaded
- **Zero 404 Guarantee**: All media files are downloaded to ensure pages render without missing resources

### **Relative Path Resolution (Fixed)**
- **Issue**: Image paths in mirrored HTML files had incorrect base/root paths when HTML files were in subdirectories
- **Solution**: Implemented proper relative path calculation using `pathdiff` crate to generate correct `../../` prefixes
- **Result**: All resource links in HTML now use proper relative paths from the HTML file location to the resource files
- **Example**: HTML at `artfestival/artists/index.html` now correctly references images as `../../mykonos-biennale-cache*/*/image.jpg`

## ğŸš€ **Smart Download Cache & Enhanced Logging**

### **Download Cache System:**
The utility implements an intelligent caching system that **prevents duplicate downloads** and significantly improves efficiency:

- **Unique File Tracking**: Each file is downloaded only once, regardless of how many pages reference it
- **Memory & Disk Cache**: Combines in-memory tracking with disk existence checks
- **Automatic Deduplication**: Prevents duplicate files in the output directory
- **Bandwidth Optimization**: Reduces unnecessary network requests

### **Enhanced Resource Logging:**
Get complete visibility into the mirroring process with detailed logging:

- **Resource Type Detection**: Automatically identifies and categorizes:
  - ğŸ–¼ï¸ **Images**: `.png`, `.jpg`, `.jpeg`, `.gif`, `.webp`, `.svg`
  - ğŸ¨ **CSS**: `.css` files and `/css/` paths
  - âš¡ **JavaScript**: `.js` files and `/js/` paths
  - ğŸ”¤ **Fonts**: `.woff`, `.woff2`, `.ttf`, `.eot`
  - ğŸ“„ **Other resources**: Any other file types

- **Processing Status**: Clear indicators for each stage:
  - `ğŸ” Processing [Type] resource:` - Shows what's being analyzed
  - `ğŸ“¥ Downloading [Type]:` - Shows what's being downloaded
  - `âœ… Downloaded [Type] to:` - Shows where files are saved
  - `â­ï¸ Skipping (already downloaded)` - Shows cache efficiency

### **Performance Benefits:**
- **Faster Mirroring**: Subsequent pages with shared resources process instantly
- **Reduced Storage**: No duplicate files in the output directory
- **Network Efficiency**: Minimal bandwidth usage through smart caching
- **Progress Visibility**: Real-time insight into what's happening during mirroring

## Installation

### Prerequisites

- Rust 1.89.0 or later
- Cargo (comes with Rust)

### Build from Source

1. Clone or download this repository
2. Navigate to the project directory
3. Build the project:

```bash
cargo build --release
```

4. The binary will be available at `target/release/website-mirror`

### Install Globally (Optional)

```bash
cargo install --path .
```

## ğŸš€ Usage Examples

```bash
# Basic mirroring
./website-mirror https://example.com

# Mirror with external resources (CSS, JS, images from other domains)
./website-mirror https://example.com --download-external

# Full recursive mirroring (all options enabled)
./website-mirror https://example.com --full-mirror

# Note: All media files are automatically downloaded to ensure no 404 errors
# The utility now includes smart caching to prevent duplicate downloads

# Custom depth and concurrency
./website-mirror https://example.com -d 5 -c 20 -o ./my_mirror

# Mirror with robots.txt bypass
./website-mirror https://example.com --ignore-robots --download-external

# High-performance mirroring
./website-mirror https://large-site.com --max-concurrent 50 --timeout 60

# Mirror only specific resource types
./website-mirror https://example.com --only-resources images,css
./website-mirror https://example.com --only-resources js
./website-mirror https://example.com --only-resources images

# Convert images to WebP for better compression
./website-mirror https://example.com --convert-to-webp
./website-mirror https://example.com --only-resources images --convert-to-webp
```

## Usage

### Basic Usage

```bash
# Mirror a simple website
./website-mirror https://example.com

# Mirror with custom output directory
./website-mirror https://example.com -o ./my_mirror

# Mirror with depth and concurrency limits
./website-mirror https://example.com -d 5 -c 20
```

### Advanced Options

```bash
./website-mirror https://example.com \
  --output-dir ./website_copy \
  --max-depth 10 \
  --max-concurrent 25 \
  --ignore-robots \
  --download-external \
  --timeout 60
```

### Command Line Options

| Option | Short | Description | Default |
|--------|-------|-------------|---------|
| `--url` | - | Target website URL (required) | - |
| `--output-dir` | -o | Output directory for mirrored files | `./mirrored_site` |
| `--max-depth` | -d | Maximum crawling depth | `3` |
| `--max-concurrent` | -c | Maximum concurrent downloads | `10` |
| `--ignore-robots` | -r | Ignore robots.txt restrictions | `false` |
| `--download-external` | -e | Download external resources | `false` |
| `--only-resources` | - | Mirror only specific resource types (images,css,js,html) | `all` |
| `--convert-to-webp` | - | Convert JPEG/PNG images to WebP format for better compression | `false` |
| `--user-agent` | -u | Custom user agent string | `WebsiteMirror/1.0` |
| `--follow-redirects` | -f | Follow HTTP redirects | `true` |
| `--timeout` | -t | Request timeout in seconds | `30` |

## Examples

### Mirror a Blog

```bash
./website-mirror https://blog.example.com -d 5 -c 15 -o ./blog_mirror
```

### Mirror with External Resources

```bash
./website-mirror https://example.com \
  --download-external \
  --max-depth 3 \
  --output-dir ./full_mirror
```

### High-Performance Mirroring

```bash
./website-mirror https://large-site.com \
  --max-concurrent 50 \
  --timeout 60 \
  --ignore-robots \
  -o ./large_site_mirror
```

## How It Works

1. **Initialization**: Sets up HTTP client with SSL certificate handling
2. **Crawling**: Starts from the base URL and discovers linked pages
3. **Resource Extraction**: Parses HTML to find CSS, JavaScript, and image files
4. **Download**: Downloads all discovered resources concurrently
5. **Link Conversion**: Converts all links to work with local file structure
6. **File Organization**: Creates directories and saves files with proper extensions

## ğŸ“ File Structure

The mirrored website will maintain a structure similar to the original. Here's an example of what gets created:

```
mirrored_site/
â”œâ”€â”€ index.html
â”œâ”€â”€ _css/
â”‚   â””â”€â”€ 2025.01/
â”‚       â””â”€â”€ iana_website.css
â”œâ”€â”€ _js/
â”‚   â”œâ”€â”€ jquery.js
â”‚   â””â”€â”€ iana.js
â”œâ”€â”€ _img/
â”‚   â””â”€â”€ 2025.01/
â”‚       â””â”€â”€ iana-logo-header.svg
â”œâ”€â”€ about/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ css/
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ bootstrap.min.css
â”œâ”€â”€ js/
â”‚   â””â”€â”€ main.js
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ logo.png
â”‚   â””â”€â”€ banner.jpg
â””â”€â”€ blog/
    â”œâ”€â”€ index.html
    â””â”€â”€ post-1/
        â””â”€â”€ index.html
```

### ğŸ¯ **The Image Download Issue - SOLVED!**

The tool now properly handles:
- **Relative URLs** - Correctly resolves paths like `/_img/logo.svg` to full URLs
- **External resources** - Downloads CSS, JavaScript, and images from other domains when `--download-external` is enabled
- **Proper file saving** - All resources are correctly saved to the local filesystem with proper directory structure

## ğŸ”§ Key Fixes & Improvements

### **Image Download Issue - RESOLVED!**
The tool now correctly downloads images, CSS, and JavaScript files by:
1. **Proper URL resolution** - Each page uses its own base URL for resource resolution
2. **External resource handling** - The `--download-external` flag properly downloads resources from other domains
3. **Correct file paths** - Resources are saved with proper directory structure

### **HTML Link Conversion**
- Automatically converts all links in downloaded HTML to work locally
- Handles relative and absolute URLs correctly
- Maintains proper file structure for offline browsing

### **Resource Management**
- Downloads CSS files with proper MIME type detection
- Downloads JavaScript files and maintains script references
- Downloads images and maintains image references
- Creates necessary directories automatically

## SSL Certificate Handling

The utility automatically handles SSL certificates using the system's trusted root certificates. It supports:
- Modern TLS versions (TLS 1.2, TLS 1.3)
- Self-signed certificates (with proper configuration)
- Certificate chain validation

## Performance Considerations

- **Concurrent Downloads**: Adjust `--max-concurrent` based on your system and network
- **Depth Limits**: Use `--max-depth` to control crawling depth and prevent infinite loops
- **Timeout Settings**: Increase `--timeout` for slow servers or large files
- **External Resources**: Enable `--download-external` only when needed

## ğŸ§ª Testing & Verification

### **Test with Real Websites**
The tool has been tested and verified with:
- **httpbin.org** - Basic HTML pages
- **example.com** - Simple static content
- **iana.org** - Complex sites with CSS, JavaScript, and images

### **Verification Commands**
```bash
# Check downloaded files
find ./mirrored_site -type f | head -20

# Verify images were downloaded
find ./mirrored_site -name "*.svg" -o -name "*.png" -o -name "*.jpg" -o -name "*.gif"

# Verify CSS and JS files
find ./mirrored_site -name "*.css" -o -name "*.js"

# Check file sizes
ls -lah ./mirrored_site/_img/* ./mirrored_site/_css/* ./mirrored_site/_js/*
```

### **Example Enhanced Logging Output:**

The new logging system provides clear visibility into the mirroring process:

```
ğŸ” Processing CSS resource: https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css
ğŸ“¥ Downloading CSS: https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css
âœ… Downloaded CSS to: ./output/font-awesome/4.3.0/css/font-awesome.min.css

ğŸ” Processing Image resource: https://s3.amazonaws.com/example.com/image.jpg
ğŸ“¥ Downloading Image: https://s3.amazonaws.com/example.com/image.jpg
âœ… Downloaded Image to: ./output/s3.amazonaws.com/example.com/image.jpg

ğŸ” Processing JavaScript resource: https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.1/jquery.min.js
ğŸ“¥ Downloading JavaScript: https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.1/jquery.min.js
âœ… Downloaded JavaScript to: ./output/ajax/libs/jquery/2.1.1/jquery.min.js

# Subsequent references to the same files show cache efficiency:
ğŸ” Processing CSS resource: https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css
â­ï¸  Skipping https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css (already downloaded to font-awesome/4.3.0/css/font-awesome.min.css)
```

This logging makes it easy to:
- **Track progress** through the mirroring process
- **Identify resource types** being downloaded
- **Monitor cache efficiency** and performance
- **Debug any issues** with specific resources

## Troubleshooting

### Common Issues

1. **Permission Denied**: Ensure you have write permissions to the output directory
2. **SSL Errors**: Check if the target site uses valid SSL certificates
3. **Timeout Errors**: Increase the timeout value for slow servers
4. **Memory Issues**: Reduce concurrent download limits for large sites
5. **Images not downloading**: Use the `--download-external` flag to download external resources

### Debug Mode

For detailed logging, you can set the `RUST_LOG` environment variable:

```bash
RUST_LOG=debug ./website-mirror https://example.com
```

## Contributing

Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Disclaimer

This tool is intended for legitimate purposes such as:
- Creating offline backups of your own websites
- Archiving public websites for research purposes
- Testing website functionality offline

Please respect website terms of service and robots.txt files when using this tool. 